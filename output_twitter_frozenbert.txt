Now we have made our own model for twitter. It differs with Goemotions BERT in:
TwitterBERT[classifier.weight] is torch.Size([2, 768]) while GoemBERT[classifier.weight] is torch.Size([4, 768])
TwitterBERT[classifier.bias] is torch.Size([2]) while GoemBERT[classifier.bias] is torch.Size([4])
***** Running training *****
  Num examples = 8000
  Num Epochs = 10
  Total train batch size = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 5000
  Logging steps = 1000
  Save steps = 1000
***** Running evaluation on dev dataset (1000 step) *****
  Num examples = 400
  Eval Batch size = 32
***** Eval results on dev dataset *****
  accuracy = 0.4975
  loss = 0.5338980922332177
  macro_f1 = 0.7517728049138184
  macro_precision = 0.6429400777424403
  macro_recall = 0.905
  micro_f1 = 0.7518172377985463
  micro_precision = 0.6429840142095915
  micro_recall = 0.905
  weighted_f1 = 0.7517728049138184
  weighted_precision = 0.6429400777424403
  weighted_recall = 0.905
Saving model checkpoint to ckpt/twitter_frozenbert/bert-base-cased-goemotions-twitter-frozenbert/checkpoint-1000
***** Running evaluation on dev dataset (2000 step) *****
  Num examples = 400
  Eval Batch size = 32
***** Eval results on dev dataset *****
  accuracy = 0.4975
  loss = 0.529120534658432
  macro_f1 = 0.7440119498943027
  macro_precision = 0.6391897467636288
  macro_recall = 0.89
  micro_f1 = 0.7439916405433648
  micro_precision = 0.6391382405745063
  micro_recall = 0.89
  weighted_f1 = 0.7440119498943027
  weighted_precision = 0.6391897467636288
  weighted_recall = 0.89
Saving model checkpoint to ckpt/twitter_frozenbert/bert-base-cased-goemotions-twitter-frozenbert/checkpoint-2000
***** Running evaluation on dev dataset (3000 step) *****
  Num examples = 400
  Eval Batch size = 32
***** Eval results on dev dataset *****
  accuracy = 0.5075
  loss = 0.529570785852579
  macro_f1 = 0.7498201470407608
  macro_precision = 0.6453146925835456
  macro_recall = 0.895
  micro_f1 = 0.7497382198952879
  micro_precision = 0.645045045045045
  micro_recall = 0.895
  weighted_f1 = 0.7498201470407608
  weighted_precision = 0.6453146925835455
  weighted_recall = 0.895
Saving model checkpoint to ckpt/twitter_frozenbert/bert-base-cased-goemotions-twitter-frozenbert/checkpoint-3000
***** Running evaluation on dev dataset (4000 step) *****
  Num examples = 400
  Eval Batch size = 32
***** Eval results on dev dataset *****
  accuracy = 0.5075
  loss = 0.5289734510275034
  macro_f1 = 0.7498574248738759
  macro_precision = 0.6454191033138401
  macro_recall = 0.895
  micro_f1 = 0.7497382198952879
  micro_precision = 0.645045045045045
  micro_recall = 0.895
  weighted_f1 = 0.7498574248738759
  weighted_precision = 0.6454191033138401
  weighted_recall = 0.895
Saving model checkpoint to ckpt/twitter_frozenbert/bert-base-cased-goemotions-twitter-frozenbert/checkpoint-4000
***** Running evaluation on dev dataset (5000 step) *****
  Num examples = 400
  Eval Batch size = 32
***** Eval results on dev dataset *****
  accuracy = 0.5025
  loss = 0.5278822550406823
  macro_f1 = 0.7469217671442141
  macro_precision = 0.6422617429684567
  macro_recall = 0.8925000000000001
  micro_f1 = 0.7468619246861926
  micro_precision = 0.6420863309352518
  micro_recall = 0.8925
  weighted_f1 = 0.7469217671442141
  weighted_precision = 0.6422617429684567
  weighted_recall = 0.8925
Saving model checkpoint to ckpt/twitter_frozenbert/bert-base-cased-goemotions-twitter-frozenbert/checkpoint-5000
 global_step = 5000, average loss = 0.5478168604195118
Evaluate the following checkpoints: ['ckpt/twitter_frozenbert/bert-base-cased-goemotions-twitter-frozenbert/checkpoint-1000', 'ckpt/twitter_frozenbert/bert-base-cased-goemotions-twitter-frozenbert/checkpoint-2000', 'ckpt/twitter_frozenbert/bert-base-cased-goemotions-twitter-frozenbert/checkpoint-3000', 'ckpt/twitter_frozenbert/bert-base-cased-goemotions-twitter-frozenbert/checkpoint-4000', 'ckpt/twitter_frozenbert/bert-base-cased-goemotions-twitter-frozenbert/checkpoint-5000']
***** Running evaluation on test dataset (1000 step) *****
  Num examples = 359
  Eval Batch size = 32
***** Eval results on test dataset *****
  accuracy = 0.6406685236768802
  loss = 0.41956330090761185
  macro_f1 = 0.8155387851045404
  macro_precision = 0.7227444304640663
  macro_recall = 0.9361147327249022
  micro_f1 = 0.8155339805825242
  micro_precision = 0.7225806451612903
  micro_recall = 0.935933147632312
  weighted_f1 = 0.815566354228087
  weighted_precision = 0.7228959628273589
  weighted_recall = 0.935933147632312
***** Running evaluation on test dataset (2000 step) *****
  Num examples = 359
  Eval Batch size = 32
***** Eval results on test dataset *****
  accuracy = 0.6518105849582173
  loss = 0.4097881441315015
  macro_f1 = 0.8195170702978603
  macro_precision = 0.7288811925914772
  macro_recall = 0.9361147327249022
  micro_f1 = 0.8195121951219512
  micro_precision = 0.7288503253796096
  micro_recall = 0.935933147632312
  weighted_f1 = 0.8194892315496337
  weighted_precision = 0.728947254730154
  weighted_recall = 0.935933147632312
***** Running evaluation on test dataset (3000 step) *****
  Num examples = 359
  Eval Batch size = 32
***** Eval results on test dataset *****
  accuracy = 0.6545961002785515
  loss = 0.4072091927131017
  macro_f1 = 0.8205238299699442
  macro_precision = 0.7306557749035625
  macro_recall = 0.9361147327249022
  micro_f1 = 0.8205128205128205
  micro_precision = 0.7304347826086957
  micro_recall = 0.935933147632312
  weighted_f1 = 0.8205656904406232
  weighted_precision = 0.730832753412683
  weighted_recall = 0.935933147632312
***** Running evaluation on test dataset (4000 step) *****
  Num examples = 359
  Eval Batch size = 32
***** Eval results on test dataset *****
  accuracy = 0.6573816155988857
  loss = 0.40756015107035637
  macro_f1 = 0.8201191375115362
  macro_precision = 0.7318376068376069
  macro_recall = 0.9332898739678401
  micro_f1 = 0.8200734394124847
  micro_precision = 0.7314410480349345
  micro_recall = 0.9331476323119777
  weighted_f1 = 0.820223135692386
  weighted_precision = 0.7320905649596458
  weighted_recall = 0.9331476323119777
***** Running evaluation on test dataset (5000 step) *****
  Num examples = 359
  Eval Batch size = 32
***** Eval results on test dataset *****
  accuracy = 0.6601671309192201
  loss = 0.4064548040429751
  macro_f1 = 0.821078431372549
  macro_precision = 0.733153277401065
  macro_recall = 0.9332898739678401
  micro_f1 = 0.821078431372549
  micro_precision = 0.7330415754923414
  micro_recall = 0.9331476323119777
  weighted_f1 = 0.8211125675897099
  weighted_precision = 0.7332954717528385
  weighted_recall = 0.9331476323119777
